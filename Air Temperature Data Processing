import pandas as pd
import numpy as np
import requests
import dask
from dask.diagnostics import ProgressBar
import plotly.graph_objects as go
from itertools import product

class TemperatureDataProcessor:
    """
    A class for fetching, processing, and editing temperature data for multiple stations.
    """

    def __init__(self, station_triplets, start_date, end_date):
        self.base_url = "https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/data"
        self.station_triplets = station_triplets
        self.start_date = start_date
        self.end_date = end_date
        self.elements = ["TOBS:*:1", "TMAX:*:1", "TMIN:*:1", "TAVG:*:1"]
        self.durations = ["HOURLY", "DAILY", "DAILY", "DAILY"]  # TOBS is HOURLY, others are DAILY

    def fetch_awdb_data(self, station_triplet, element, duration):
        """Fetch data for a single station and element."""
        try:
            params = {
                "stationTriplets": station_triplet,
                "beginDate": self.start_date,
                "endDate": self.end_date,
                "duration": duration,
                "elements": element,
                "centralTendencyType": "NONE"
            }
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error fetching {element} for {station_triplet}: {e}")
            return None

    def preprocess_data(self, data_json, station_id, element, duration):
        """Process data and prefix columns with station ID."""
        if not data_json or not isinstance(data_json, list):
            return pd.DataFrame()

        try:
            flat_data = [val for sublist in data_json[0].get('data', []) for val in sublist.get('values', [])]
            df = pd.DataFrame(flat_data)
            df['date'] = pd.to_datetime(df['date'], errors='coerce')

            if duration == "HOURLY":
                df.set_index('date', inplace=True)
            else:
                df['date'] = df['date'].dt.date  # Normalize to daily
                df.set_index('date', inplace=True)

            df['value'] = df['value'].astype(float)
            element_label = element.split(":")[0]
            df.rename(columns={'value': f"{station_id}_{element_label}"}, inplace=True)
            return df
        except Exception as e:
            print(f"Error preprocessing {station_id}_{element}: {e}")
            return pd.DataFrame()

    def parallel_fetch(self):
        """Fetch and process data for all stations/elements in parallel."""
        tasks = []
        for station, (element, duration) in product(self.station_triplets, zip(self.elements, self.durations)):
            tasks.append(dask.delayed(self.fetch_awdb_data)(station, element, duration))

        with ProgressBar():
            results = dask.compute(*tasks, num_workers=4)

        # Map results to stations and elements
        dfs = []
        for idx, (station, (element, duration)) in enumerate(product(self.station_triplets, zip(self.elements, self.durations))):
            result = results[idx]
            element_label = element.split(":")[0]
            df = self.preprocess_data(result, station, element, duration)
            if not df.empty:
                dfs.append(df)

        # Outer join all DataFrames on date
        combined_df = pd.concat(dfs, axis=1).sort_index()
        combined_df = combined_df.loc[~combined_df.index.duplicated(keep='first')]
        return combined_df

    def apply_tobs_validation(self, df):
        """Validate TMAX, TMIN, and TAVG using TOBS hourly data."""
        df.index = pd.to_datetime(df.index)

        # Compute TOBS daily statistics
        df["TOBS_MAX"] = df.filter(like="TOBS").resample("D").max()
        df["TOBS_MIN"] = df.filter(like="TOBS").resample("D").min()
        df["TOBS_MEAN"] = df.filter(like="TOBS").resample("D").mean()

        for col in df.columns:
            if "_raw" not in col and "_qc_flag" not in col:
                station_element = col.split("_")
                station_id = "_".join(station_element[:-1])
                element = station_element[-1]

                if element in ["TMAX", "TMIN", "TAVG"]:
                    # Preserve raw data and set QC flags
                    raw_col = f"{station_id}_{element}_raw"
                    qc_col = f"{station_id}_{element}_qc_flag"
                    df[raw_col] = df[col]
                    df[qc_col] = "V"

                    # Apply TOBS-based validation
                    if element == "TMAX":
                        invalid = df[col] > (df["TOBS_MAX"] + 5)
                    elif element == "TMIN":
                        invalid = df[col] < (df["TOBS_MIN"] - 5)
                    elif element == "TAVG":
                        invalid = (df[col] > df["TOBS_MEAN"] + 5) | (df[col] < df["TOBS_MEAN"] - 5)

                    # Flag and remove invalid values
                    df.loc[invalid, qc_col] = "S"
                    df.loc[invalid, col] = np.nan

        return df

    def plot_comparison(self, raw_df, edited_df, title):
        """Plot before and after cleaning for all stations/elements."""
        fig = go.Figure()
        for col in edited_df.columns:
            if "_qc_flag" not in col and "_raw" not in col and "TOBS" not in col:
                station_element = col.split("_")
                station_id = "_".join(station_element[:-1])
                element = station_element[-1]
                raw_col = f"{station_id}_{element}_raw"

                # Plot raw and edited data
                fig.add_trace(go.Scatter(
                    x=raw_df.index, y=raw_df[raw_col],
                    mode="lines", name=f"{station_id} Raw {element}",
                    line=dict(color='lightgray')
                ))
                fig.add_trace(go.Scatter(
                    x=edited_df.index, y=edited_df[col],
                    mode="lines", name=f"{station_id} Edited {element}"
                ))
        fig.update_layout(title=title, xaxis_title="Date", yaxis_title="Temperature (Â°F)")
        fig.show()

# ---------------------- SCRIPT EXECUTION ----------------------
stations = ["840:CO:SNTL", "874:CO:SNTL"]  # Multiple station triplets
start_date = "1970-01-01 00:00"
end_date = "2025-01-29 00:00"

processor = TemperatureDataProcessor(stations, start_date, end_date)
raw_data = processor.parallel_fetch()
edited_data = processor.apply_tobs_validation(raw_data)
processor.plot_comparison(raw_data, edited_data, "Multi-Site Temperature Comparison")

# Export to CSV
edited_data.to_csv("multi_site_temperature_edited.csv")
print("Data saved to 'multi_site_temperature_edited.csv'.")
