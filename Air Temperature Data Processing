import pandas as pd
import numpy as np
import requests
import dask
from dask.diagnostics import ProgressBar
import plotly.graph_objects as go

class TemperatureDataProcessor:
    """
    A class for fetching, processing, cleaning, and exporting temperature data 
    from USDA NRCS AWDB API.
    """

    def __init__(self, station_triplets, start_date, end_date, db_name="temperature_data.db"):
        """
        Initializes the TemperatureDataProcessor.

        Parameters:
        - station_triplets (list): List of station identifiers.
        - start_date (str): Start date in YYYY-MM-DD format.
        - end_date (str): End date in YYYY-MM-DD format.
        - db_name (str): SQLite database name for data storage.
        """
        self.base_url = "https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/data"
        self.station_triplets = station_triplets
        self.start_date = start_date
        self.end_date = end_date
        self.db_name = db_name
        self.elements = ["TOBS:*:1", "TMAX:*:1", "TMIN:*:1", "TAVG:*:1"]  # Elements to fetch
        self.durations = ["HOURLY", "DAILY", "DAILY", "DAILY"]  # Corresponding durations for elements

    def fetch_awdb_data(self, station_triplet, element, duration):
        """
        Fetches temperature data from the AWDB API.

        Parameters:
        - station_triplet (str): Station identifier.
        - element (str): Temperature element (e.g., TOBS, TMAX).
        - duration (str): Duration type (e.g., DAILY, HOURLY).

        Returns:
        - JSON response containing data or None if failed.
        """
        params = {
            "stationTriplets": station_triplet,
            "beginDate": self.start_date,
            "endDate": self.end_date,
            "duration": duration,
            "elements": element,
            "centralTendencyType": "NONE"
        }
        response = requests.get(self.base_url, params=params)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"Error fetching {element} data: {response.status_code}")
            return None

    def preprocess_data(self, data_json, label, is_hourly=False):
        """
        Converts API JSON response to a pandas DataFrame and aligns timestamps.

        Parameters:
        - data_json (dict): JSON response from API.
        - label (str): Column name for temperature element.
        - is_hourly (bool): If True, data is hourly and must be aggregated.

        Returns:
        - Pandas DataFrame with corrected timestamps.
        """
        if not data_json or not isinstance(data_json, list):
            return pd.DataFrame()

        try:
            flat_data = [val for sublist in data_json[0].get('data', []) for val in sublist.get('values', [])]
            if not flat_data:
                return pd.DataFrame()
            
            df = pd.DataFrame(flat_data)
            df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Convert timestamps
            df['value'] = df['value'].astype(float)
            df.rename(columns={'value': label}, inplace=True)

            # If hourly data (TOBS), aggregate to daily
            if is_hourly:
                df['date'] = df['date'].dt.date  # Convert to YYYY-MM-DD
                df = df.groupby('date').mean()  # Aggregate hourly to daily

            df.set_index('date', inplace=True)
            return df
        except Exception:
            return pd.DataFrame()

    def parallel_fetch(self):
        """
        Fetches data for all temperature elements in parallel.

        Returns:
        - Dictionary containing processed data for each element.
        """
        fetch_tasks = []
        for station in self.station_triplets:
            for element, duration in zip(self.elements, self.durations):
                fetch_tasks.append(dask.delayed(self.fetch_awdb_data)(station, element, duration))

        with ProgressBar():
            results = dask.compute(*fetch_tasks, num_workers=4)

        processed_data = {}
        for (element, result) in zip(self.elements, results):
            label = element.split(":")[0]
            is_hourly = (label == "TOBS")  # Mark TOBS as hourly
            processed_data[label] = self.preprocess_data(result, label, is_hourly=is_hourly)

        return processed_data

    def clean_temp_data(self, df, temp_range=(-50, 130)):
        """
        Cleans temperature data by removing extreme values and flagging anomalies.

        Parameters:
        - df (DataFrame): Temperature dataset.
        - temp_range (tuple): Valid temperature range.

        Returns:
        - Cleaned DataFrame with QC flags.
        """
        df.index = pd.to_datetime(df.index).normalize()

        # Preserve original values and set default QC flags
        for col in ["TMAX", "TMIN", "TAVG", "TOBS"]:
            if col in df.columns:
                df[f"{col}_raw"] = df[col]
                df[f"{col}_qc_flag"] = "V"

        # Flag and remove extreme temperature values
        for col in ["TMAX", "TMIN", "TAVG", "TOBS"]:
            if col in df.columns:
                outliers = (df[col] < temp_range[0]) | (df[col] > temp_range[1])
                df.loc[outliers, f"{col}_qc_flag"] = "E"
                df.loc[outliers, col] = np.nan  # Replace with NaN

        return df

    def export_to_csv(self, df, filename="temperature_cleaned.csv"):
        """
        Exports cleaned data to a CSV file.

        Parameters:
        - df (DataFrame): Cleaned temperature data.
        - filename (str): CSV filename.
        """
        df.to_csv(filename, index=True)
        print(f"Cleaned data saved to {filename}")

    def plot_raw_data(self, df):
        """
        Plots raw temperature data.

        Parameters:
        - df (DataFrame): Data to plot.
        """
        if df is None or df.empty:
            print("No data to plot.")
            return

        fig = go.Figure()
        for col in ["TOBS", "TMAX", "TMIN", "TAVG"]:
            if col in df.columns:
                fig.add_trace(go.Scatter(x=df.index, y=df[col], mode="lines", name=col))

        fig.update_layout(title="Raw Temperature Data", xaxis_title="Date", yaxis_title="Temperature (Â°F)")
        fig.show()

# ---------------------- SCRIPT EXECUTION ----------------------
station_triplets = ["737:CO:SNTL"]
start_date = "2024-01-01"
end_date = "2024-12-31"

# Initialize processor
temp_processor = TemperatureDataProcessor(station_triplets, start_date, end_date)

print("\n--- Fetching Data ---\n")
temp_data = temp_processor.parallel_fetch()

# Merge datasets using daily timestamps
df_combined = None
for key, df in temp_data.items():
    if not df.empty:
        df.index = pd.to_datetime(df.index)  # Ensure index is in datetime format
        df.index = df.index.normalize()  # Normalize timestamps to YYYY-MM-DD

        if df_combined is None:
            df_combined = df
        else:
            df_combined = df_combined.join(df, how="outer")

df_combined.sort_index(inplace=True)  # Ensure correct chronological order

# Clean data
df_cleaned = temp_processor.clean_temp_data(df_combined)

# Export to CSV
columns_to_export = [
    "AUX_raw", "TMAX", "TMAX_qc_flag",
    "TMIN_raw", "TMIN", "TMIN_qc_flag",
    "TAVG_raw", "TAVG", "TAVG_qc_flag",
    "TOBS_raw", "TOBS", "TOBS_qc_flag",
]
df_cleaned[columns_to_export].to_csv("temperature_cleaned.csv", index=True)
print("Cleaned data saved to 'temperature_cleaned.csv'.")

# Plot raw data
print("\n--- Plotting Raw Data ---\n")
temp_processor.plot_raw_data(df_combined)
