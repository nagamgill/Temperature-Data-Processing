import pandas as pd
import numpy as np
import requests
import dask
from dask.diagnostics import ProgressBar
import plotly.graph_objects as go
import sqlite3

class TemperatureDataProcessor:
    """
    A class for fetching, processing, cleaning, and exporting temperature data 
    from USDA NRCS AWDB API.
    """

    def __init__(self, station_triplets, start_date, end_date, db_name="temperature_data.db"):
        """
        Initializes the TemperatureDataProcessor.

        Parameters:
        - station_triplets (list): List of station identifiers.
        - start_date (str): Start date in YYYY-MM-DD format.
        - end_date (str): End date in YYYY-MM-DD format.
        - db_name (str): SQLite database name for data storage.
        """
        self.base_url = "https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/data"
        self.station_triplets = station_triplets
        self.start_date = start_date
        self.end_date = end_date
        self.db_name = db_name
        self.elements = ["TOBS:*:1", "TMAX:*:1", "TMIN:*:1", "TAVG:*:1"]  # Elements to fetch
        self.durations = ["DAILY", "DAILY", "DAILY", "DAILY"]  # Ensures correct duration mapping

    def fetch_awdb_data(self, station_triplet, element, duration):
        """
        Fetches temperature data from the AWDB API.

        Parameters:
        - station_triplet (str): Station identifier.
        - element (str): Temperature element (e.g., TOBS, TMAX).
        - duration (str): Duration type (e.g., DAILY, HOURLY).

        Returns:
        - JSON response containing data or None if failed.
        """
        params = {
            "stationTriplets": station_triplet,
            "beginDate": self.start_date,
            "endDate": self.end_date,
            "duration": duration,
            "elements": element,
            "centralTendencyType": "NONE"
        }
        response = requests.get(self.base_url, params=params)

        print(f"\nFetching {element} for {station_triplet} - Status Code: {response.status_code}")
        print(f"API Request URL: {response.url}")

        if response.status_code == 200:
            data = response.json()
            return data
        else:
            print(f"Error fetching {element} data for {station_triplet}: {response.status_code}")
            return None

    def preprocess_data(self, data_json, label):
        """
        Converts API JSON response to a pandas DataFrame.

        Parameters:
        - data_json (dict): JSON response from API.
        - label (str): Column name for temperature element.

        Returns:
        - Pandas DataFrame containing processed data.
        """
        if not data_json or not isinstance(data_json, list):
            return pd.DataFrame()

        try:
            flat_data = [val for sublist in data_json[0].get('data', []) for val in sublist.get('values', [])]
            if not flat_data:
                return pd.DataFrame()
            
            df = pd.DataFrame(flat_data)
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            df.set_index('date', inplace=True)
            df['value'] = df['value'].astype(float)
            df.rename(columns={'value': label}, inplace=True)
            return df
        except Exception:
            return pd.DataFrame()

    def parallel_fetch(self):
        """
        Fetches data for all temperature elements in parallel.

        Returns:
        - Dictionary containing processed data for each element.
        """
        fetch_tasks = []
        for station in self.station_triplets:
            for element, duration in zip(self.elements, self.durations):
                fetch_tasks.append(dask.delayed(self.fetch_awdb_data)(station, element, duration))

        with ProgressBar():
            results = dask.compute(*fetch_tasks, num_workers=4)

        processed_data = {}
        for (element, result) in zip(self.elements, results):
            label = element.split(":")[0]
            processed_data[label] = self.preprocess_data(result, label)

        return processed_data

    def clean_temp_data(self, df, temp_range=(-50, 130)):
        """
        Cleans temperature data by removing extreme values and flagging anomalies.

        Parameters:
        - df (DataFrame): Temperature dataset.
        - temp_range (tuple): Valid temperature range.

        Returns:
        - Cleaned DataFrame with QC flags.
        """
        df.index = pd.to_datetime(df.index).normalize()

        # Preserve original values and set default QC flags
        for col in ["TMAX", "TMIN", "TAVG", "TOBS"]:
            if col in df.columns:
                df[f"{col}_raw"] = df[col]  # Change from _original to _raw
                df[f"{col}_qc_flag"] = "V"  # Default flag is valid

        # Flag and remove extreme temperature values
        for col in ["TMAX", "TMIN", "TAVG", "TOBS"]:
            if col in df.columns:
                outliers = (df[col] < temp_range[0]) | (df[col] > temp_range[1])
                df.loc[outliers, f"{col}_qc_flag"] = "E"
                df.loc[outliers, col] = ""  # Replace with empty string instead of NaN

        return df

    def export_to_csv(self, df, filename="temperature_cleaned.csv"):
        """
        Exports cleaned data to a CSV file.

        Parameters:
        - df (DataFrame): Cleaned temperature data.
        - filename (str): CSV filename.
        """
        df.to_csv(filename, index=True)
        print(f"Cleaned data saved to {filename}")

# ---------------------- SCRIPT EXECUTION ----------------------
station_triplets = ["737:CO:SNTL"]
start_date = "2024-01-01"
end_date = "2024-12-31"

# Initialize processor
temp_processor = TemperatureDataProcessor(station_triplets, start_date, end_date)

print("\n--- Fetching Data ---\n")
temp_data = temp_processor.parallel_fetch()

# Merge datasets
df_combined = None
for key, df in temp_data.items():
    if not df.empty:
        if df_combined is None:
            df_combined = df
        else:
            df_combined = df_combined.join(df, how="outer")

# Clean data
df_cleaned = temp_processor.clean_temp_data(df_combined)

# Export to CSV
columns_to_export = [
    "TMAX_raw", "TMAX", "TMAX_qc_flag",
    "TMIN_raw", "TMIN", "TMIN_qc_flag",
    "TAVG_raw", "TAVG", "TAVG_qc_flag",
    "TOBS_raw", "TOBS", "TOBS_qc_flag",
]
df_cleaned[columns_to_export].to_csv("temperature_cleaned.csv", index=True)
print("Cleaned data saved to 'temperature_cleaned.csv'.")
